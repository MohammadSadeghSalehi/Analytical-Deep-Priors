{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the ADP and modified ADP frameworks for the deblurring problem with IFT based algorithms\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.autograd.functional import jacobian\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.autograd.functional import jacobian\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Define the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Bilevel IFT ADP with smoothed TV\n",
    "theta = torch.tensor([-2.95, -10.0, -12], requires_grad=True)\n",
    "\n",
    "\n",
    "stepsize = 5*1e-3\n",
    "losslist = []\n",
    "\n",
    "class ADP_IFT:\n",
    "    def __init__(self,A,B,x, x0,y_delta,modified = True, inexact = False,max_iter = 100, eps = 1e-12) -> None:\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.ydelta = y_delta\n",
    "        self.x = x.unsqueeze(1)\n",
    "        self.x0 = x0\n",
    "        self.modified = modified\n",
    "        self.max_iter = max_iter\n",
    "        self.inexact = inexact\n",
    "        self.eps = eps\n",
    "    def TV(self,x, nu):\n",
    "        return (torch.sum(torch.sqrt((x[1:]-x[:-1])**2+ nu**2)))\n",
    "    def TV2d(self,x, nu):\n",
    "        return (torch.sum(torch.sqrt((x[:,1:]-x[:,:-1])**2+ nu**2)) + torch.sum(torch.sqrt((x[1:,:]-x[-1,:])**2+ nu**2)))\n",
    "    def phi(self,x,B):\n",
    "        if self.modified:\n",
    "            return (0.5 *torch.linalg.norm(torch.matmul(B,torch.matmul(self.A,x)- self.ydelta).float())**2 + torch.exp(theta[0]).float() * self.TV(x,torch.exp(theta[1])) + torch.exp(theta[2]).float() * torch.linalg.norm(x)**2 ).float()\n",
    "        return (0.5 *torch.linalg.norm(torch.matmul(B,x)- self.ydelta.float())**2 + torch.exp(theta[0]).float() * self.TV(x,torch.exp(theta[1]))).float() + torch.exp(theta[2]).float() * torch.linalg.norm(x)**2\n",
    "    def Hess(self,x,B,d):\n",
    "        x.requires_grad_(True)\n",
    "        out = self.phi(x,B).float()\n",
    "        grad_x = torch.autograd.grad(outputs=out, inputs=x, grad_outputs=None, create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0] #first get grad using autograd\n",
    "        hvp = torch.autograd.grad(outputs=grad_x, inputs=x, grad_outputs=d, create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "        return hvp.detach()\n",
    "    def Jac(self,x,B,d):\n",
    "        x.requires_grad_(True)\n",
    "        B.requires_grad_(True)\n",
    "        out = self.phi(x,B)\n",
    "        grad_x = torch.autograd.grad(outputs=out, inputs=x, grad_outputs=None, create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0] #first get grad using autograd\n",
    "        gradvp = torch.tensordot(grad_x.flatten(),d,dims=([0],[0]))\n",
    "        jvp = torch.autograd.grad(outputs=gradvp, inputs=B, grad_outputs=torch.ones(gradvp.shape).requires_grad_(True), create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "        return jvp.detach()\n",
    "    def CG(self,x,B,b,tol):\n",
    "        iteration = 0\n",
    "        r = b\n",
    "        p = r\n",
    "        rsold = float(torch.linalg.norm(r)**2)\n",
    "        solution = torch.zeros(x.shape)\n",
    "        while torch.linalg.norm(r)>tol and iteration < 2000:\n",
    "            Ap = torch.tensor(self.Hess(x,B,p), dtype=torch.float32)\n",
    "            alpha = rsold/torch.tensordot(p,Ap, dims=([0],[0]))\n",
    "            solution = solution + alpha*p\n",
    "            r = r - alpha*Ap\n",
    "            rsnew = torch.linalg.norm(r)**2\n",
    "            if torch.sqrt(rsnew) < tol:\n",
    "                return solution\n",
    "            p = r + (rsnew/rsold)*p\n",
    "            rsold = rsnew\n",
    "            iteration += 1\n",
    "        return solution\n",
    "    def lbfgs(self,x,B, tol, max_iter= 10000):\n",
    "        lbfgs_optimiser= torch.optim.LBFGS([x], lr=0.05, max_iter=max_iter, max_eval=None, tolerance_grad=tol, tolerance_change=tol, history_size=100, line_search_fn=None)\n",
    "        def closure():\n",
    "            lbfgs_optimiser.zero_grad()\n",
    "            loss = self.phi(x,B)\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "        lbfgs_optimiser.step(closure)\n",
    "        return x\n",
    "    def Upper_level(self,x_hat):\n",
    "        print (\"dicrepancy \", torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta))\n",
    "        if self.modified:\n",
    "            return 0.5 * torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta)**2+ 0.05*torch.linalg.norm((-torch.eye(self.A.shape[0])+self.B))**2\n",
    "        return 0.5 * torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta)**2+ 0.05*torch.linalg.norm((-self.A+self.B))**2\n",
    "    def solver(self):\n",
    "        stochastic = False\n",
    "        eps = self.eps\n",
    "        x_hat = self.x0\n",
    "        Bk = torch.nn.parameter.Parameter(data=self.B.clone().detach(), requires_grad=True)\n",
    "        optimiser = torch.optim.Adam([Bk], lr= 7*1e-4, betas=(0.9, 0.999), eps=1e-10, weight_decay=0, amsgrad=True)\n",
    "        dp = torch.linalg.norm(torch.matmul(self.A,self.x0)- self.ydelta)\n",
    "        print (\"initial discrepancy \", dp)\n",
    "        for k in range(self.max_iter):\n",
    "            x_hat = self.lbfgs(x_hat,Bk, eps)\n",
    "            losslist.append(self.Upper_level(x_hat))\n",
    "            if k%100 == 0:\n",
    "                print('loss at iteration', k, 'is', losslist[-1].item())\n",
    "            q = self.CG(x_hat,Bk,torch.matmul(self.A.transpose(0,1),torch.matmul(self.A,x_hat)-self.ydelta),eps)\n",
    "            p = - self.Jac(x_hat,Bk,q) + 0.1*Bk.T@(-torch.eye(self.A.shape[0])+Bk)\n",
    "\n",
    "            if stochastic:\n",
    "                with torch.no_grad():\n",
    "                    param_shape = Bk.shape\n",
    "                    Bk.grad = (p.reshape(param_shape))\n",
    "                optimiser.step(lambda : self.Upper_level(x_hat))\n",
    "                if (k+1)%20 == 0:\n",
    "                    optimiser.param_groups[0]['lr'] *= 0.9\n",
    "            else:\n",
    "                if self.inexact:\n",
    "                    eps = max(eps * 0.9, 1e-12)\n",
    "                stepsize = 1e-5\n",
    "                Bk = Bk - stepsize * p\n",
    "            if (torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta) ) <= dp:\n",
    "                break\n",
    "\n",
    "            self.B = Bk\n",
    "        return self.lbfgs(x_hat,Bk, 1e-14, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete setting\n",
    "h=0.01\n",
    "t=torch.arange(-1+h/2,1,h,dtype=torch.float)\n",
    "bsp = 0\n",
    "# torch.manual_seed(0)\n",
    "if bsp == 0:\n",
    "    x = 1*(t>-0.6) - 0.7*(t>-0.5) - 0.3*(t>0) + 0.7*(t>0.2) - 0.7*(t>0.5)\n",
    "elif bsp == 1:\n",
    "    x = 0.8*torch.exp(-32*(t+0.3)**2) + 0.4*torch.exp(-16*(t-0.1)**2)\n",
    "elif bsp == 2:\n",
    "    x = (t+0.7)*(t>-0.7) - (2*t + 1)*(t>-0.5) + (t + 0.3)*(t>-0.3) + (0.9-t)*(t>-0.1) \\\n",
    "        + (t-0.2)*(t>0.2) - 0.7*(t>0.4) + (3*t-1.8)*(t>0.6) - (3*t-1.8)*(t>0.8)\n",
    "A = torch.zeros((x.shape[0],x.shape[0]), dtype=torch.float)\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[0]):\n",
    "        A[i,j] = 1/(np.sqrt(2*np.pi)*5)*np.exp(-1/50*(i-j)**2)\n",
    "B = torch.eye(A.shape[0], dtype=torch.float)\n",
    "y = torch.matmul(A,x.reshape(x.shape[0],1))\n",
    "\n",
    "eta = 0.005*torch.randn(y.shape, dtype=y.dtype)\n",
    "print('noise', torch.linalg.norm(eta))\n",
    "ydelta = y + eta\n",
    "\n",
    "# Starting point\n",
    "M = torch.matmul(A.transpose(0,1),A) + 0.0015 *torch.eye(x.shape[0])\n",
    "b = torch.matmul(A.transpose(0,1),ydelta)\n",
    "x_tik = torch.linalg.solve(M,b)\n",
    "plt.plot(t,x, label='Ground truth')\n",
    "plt.plot(t,ydelta, label='Blurry and noisy data')\n",
    "plt.plot(t,x_tik, label='Tikhonov')\n",
    "plt.legend()\n",
    "plt.savefig('G_B_T.png', dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(x, x_hat):\n",
    "    mse = torch.mean((x-x_hat)**2)\n",
    "    return 10*torch.log10(1/mse)\n",
    "# xtik = torch.randn(x.shape, dtype=x.dtype)\n",
    "x_start = torch.clone(x_tik)\n",
    "model = ADP_IFT(A, B, x, x_start, ydelta, max_iter = 1000, inexact = True, eps= 1e-1)\n",
    "x_k = model.solver().detach().numpy()\n",
    "torch.save(model.B, 'B_Mod.pt')\n",
    "np.save('ADPIFT2.npy', x_k)\n",
    "# x_k = np.load('ADPIFT2.npy')\n",
    "plt.plot(t,x_k, label='Modified ADP_IFT', linestyle='--', color='red')\n",
    "plt.plot(t,x, label='Ground truth')\n",
    "print('PSNR of modified ADP_IFT is', psnr(x,x_k).item())\n",
    "\n",
    "x_start = torch.clone(x_tik)\n",
    "B = A.clone().detach()\n",
    "model2 = ADP_IFT(A, B, x, x_tik, ydelta,modified= False,inexact = True, max_iter=1000, eps= 1e-1)\n",
    "x_k = model2.solver().detach().numpy()\n",
    "np.save('ADP2.npy', x_k)\n",
    "torch.save(model.B, 'B.pt')\n",
    "# x_k = np.load('ADP.npy')\n",
    "plt.plot(t,x_k, label='ADP_IFT', linestyle='dotted', color='green')\n",
    "plt.legend()\n",
    "print('PSNR of ADP_IFT is', psnr(x,x_k).item())\n",
    "plt.savefig('ADPvsModifiedADP2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#\n",
    "# ADP solved by an unrolled like algorithm\n",
    "#\n",
    "#################################\n",
    "def proxl1(x, alph):\n",
    "    return F.relu(F.relu(x)-alph) - F.relu(F.relu(-x)-alph)\n",
    "Bk = A\n",
    "# Bk = 1.1*torch.eye(xk.shape[0], dtype=A.dtype) - 0.1*A\n",
    "\n",
    "al_adp_l1=  0.108\n",
    "al_adp_l2= 0.027\n",
    "lamb_B=0.1\n",
    "lamb_x=0.1\n",
    "lamb = 0.05\n",
    "xk = x_tik\n",
    "for k in range(10000):\n",
    "    xkn=xk-lamb*(torch.matmul(A.transpose(0,1),torch.matmul(A,xk)-ydelta))-lamb*0.027*xk\n",
    "    xkn = proxl1(xkn,0.108*lamb)\n",
    "    if abs(xkn-xk).max()<0.000001:\n",
    "        print(k)\n",
    "        break\n",
    "    xk=xkn\n",
    "    \n",
    "eye=torch.eye(xk.shape[0],dtype=xk.dtype)\n",
    "# Elastic net ADP\n",
    "for k in range(1,7000):\n",
    "    \n",
    "    for j in range(500):\n",
    "        #compute x(B) with a classical proximal gradient method\n",
    "        xkn=xk-lamb_x*(torch.matmul(Bk.transpose(0,1),torch.matmul(Bk,xk)-ydelta))-lamb_x*al_adp_l2*xk\n",
    "        xkn=proxl1(xkn,al_adp_l1*lamb_x)\n",
    "        if (abs(xkn-xk)).sum()<0.000001:\n",
    "            break\n",
    "        xk=xkn\n",
    "\n",
    "    #compute the gradient of x(B) w.r.t. B\n",
    "    Axy = torch.matmul(A, xkn) - ydelta\n",
    "    xi = torch.matmul(A.transpose(0,1),Axy)\n",
    "\n",
    "    step = (1-al_adp_l2)*xkn - torch.matmul(Bk.transpose(0,1), torch.matmul(Bk,xkn)- ydelta) \n",
    "\n",
    "    IdBB = (1-al_adp_l2)*eye - torch.matmul(Bk.transpose(0,1),Bk)\n",
    "    for i in range(step.shape[0]):\n",
    "        if abs(step[i])<al_adp_l1:\n",
    "            IdBB[i,:] = 0 \n",
    "\n",
    "    v= torch.linalg.solve(eye - IdBB,xi)\n",
    "\n",
    "    for i in range(step.shape[0]):\n",
    "        if abs(step[i])<al_adp_l1:\n",
    "            v[i,:] = 0 \n",
    "\n",
    "    first = h*torch.matmul(torch.matmul(Bk,xk),v.transpose(0,1))\n",
    "    second = h*torch.matmul(torch.matmul(Bk,v),xk.transpose(0,1))\n",
    "    third = h*torch.matmul(ydelta,v.transpose(0,1))\n",
    "    # fourth = 2 *0.05*B.T @(B- A)\n",
    "\n",
    "    gradB = -first-second+third\n",
    "\n",
    "    Bk = Bk - lamb_B*gradB\n",
    "\n",
    "plt.plot(t,xk.detach().numpy(), label='ADP', linestyle='dotted', color='green')\n",
    "plt.plot(t,x, label='Ground truth')\n",
    "print('PSNR of ADP is', psnr(x,xk).item())\n",
    "# Elastic net ADP modified\n",
    "xk = x_tik\n",
    "Bk = torch.eye(x_tik.shape[0], dtype=A.dtype)\n",
    "for k in range(10000):\n",
    "    xkn=xk-lamb*(torch.matmul(A.transpose(0,1),torch.matmul(A,xk)-ydelta))-lamb*0.027*xk\n",
    "    xkn = proxl1(xkn,0.108*lamb)\n",
    "    if abs(xkn-xk).max()<0.000001:\n",
    "        print(k)\n",
    "        break\n",
    "    xk=xkn\n",
    "    \n",
    "for k in range(1,7000):\n",
    "    for j in range(500):\n",
    "        #compute x(B) with a classical proximal gradient method\n",
    "        xkn=xk-lamb_x*(torch.matmul(A.T@Bk.transpose(0,1),torch.matmul(Bk,A@xk-ydelta)))-lamb_x*al_adp_l2*xk\n",
    "        xkn=proxl1(xkn,al_adp_l1*lamb_x)\n",
    "        if (abs(xkn-xk)).sum()<0.000001:\n",
    "            break\n",
    "        xk=xkn\n",
    "\n",
    "    #compute the gradient of x(B) w.r.t. B\n",
    "    Axy = torch.matmul(A, xkn) - ydelta\n",
    "    xi = torch.matmul(A.transpose(0,1),Axy)\n",
    "\n",
    "    step = (1-al_adp_l2)*xkn - torch.matmul(A.T@Bk.transpose(0,1), torch.matmul(Bk,A@xkn- ydelta)) \n",
    "\n",
    "    IdBB = (1-al_adp_l2)*eye - torch.matmul(A.T@Bk.transpose(0,1),Bk@A)\n",
    "    for i in range(step.shape[0]):\n",
    "        if abs(step[i])<al_adp_l1:\n",
    "            IdBB[i,:] = 0 \n",
    "\n",
    "    v= torch.linalg.solve(eye - IdBB,xi)\n",
    "\n",
    "    for i in range(step.shape[0]):\n",
    "        if abs(step[i])<al_adp_l1:\n",
    "            v[i,:] = 0 \n",
    "\n",
    "    first = h* 2*((xk.T@B)@A.T)@B.T@x @v.T\n",
    "    second = h* (xk.T@B.T)@A.T@ydelta @v.T\n",
    "    third = 2 *0.05*B.T @(B- eye)\n",
    "    gradB = -first-second+third\n",
    "\n",
    "    Bk = Bk - lamb_B*gradB\n",
    "plt.plot(t,xk.detach().numpy(), label='Modified ADP', linestyle='dashed', color='red')\n",
    "plt.legend()\n",
    "plt.savefig('ADPvsModifiedADP_ElasticNet.png', dpi=300)\n",
    "print('PSNR of ADP modified: ', psnr(xk.detach(), x))\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the ADP and modified ADP frameworks for the deblurring problem with IFT based algorithm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.autograd.functional import jacobian\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import hessian\n",
    "from torch.autograd.functional import jacobian\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the transformation to be applied to images\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Bilevel IFT ADP with smoothed TV\n",
    "# theta = torch.tensor([-7.0, -7.9, -12], requires_grad=True)\n",
    "theta = torch.tensor([-8.7, -11.7, -12], requires_grad=True)\n",
    "\n",
    "stepsize = 5*1e-3\n",
    "losslist = []\n",
    "\n",
    "class ADP_IFT:\n",
    "    def __init__(self,A,B,x, x0,y_delta,modified = True, inexact = False,max_iter = 100, eps = 1e-12) -> None:\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.ydelta = y_delta\n",
    "        self.x = x.unsqueeze(1)\n",
    "        self.x0 = x0\n",
    "        self.modified = modified\n",
    "        self.max_iter = max_iter\n",
    "        self.inexact = inexact\n",
    "        self.eps = eps\n",
    "    def TV(self,x, nu):\n",
    "        return (torch.sum(torch.sqrt((x[1:]-x[:-1])**2+ nu**2)))\n",
    "    def TV2d(self,x, nu):\n",
    "        tv_x = (x[1:,:]-x[:-1,:])**2\n",
    "        tv_y = (x[:,1:]-x[:,:-1])**2\n",
    "        tv = torch.sum(torch.sqrt(torch.flatten(tv_x) + torch.flatten(tv_y)+ nu**2))\n",
    "        return tv\n",
    "    def phi(self,x,B):\n",
    "        if self.modified:\n",
    "            return (0.5 *torch.linalg.norm(torch.matmul(B,torch.matmul(self.A,x)- self.ydelta).float())**2 + torch.exp(theta[0]).float() * self.TV2d(x,torch.exp(theta[1])) + torch.exp(theta[2]).float() * torch.linalg.norm(x)**2 ).float()\n",
    "        return (0.5 *torch.linalg.norm(torch.matmul(B,x)- self.ydelta.float())**2 + torch.exp(theta[0]).float() * self.TV2d(x,torch.exp(theta[1]))).float() + torch.exp(theta[2]).float() * torch.linalg.norm(x)**2\n",
    "    def Hess(self,x,B,d):\n",
    "        x.requires_grad_(True)\n",
    "        out = self.phi(x,B).float()\n",
    "        grad_x = torch.autograd.grad(outputs=out, inputs=x, grad_outputs=None, create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0] #first get grad using autograd\n",
    "        hvp = torch.autograd.grad(outputs=grad_x, inputs=x, grad_outputs=d, create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "        return hvp.detach()\n",
    "    def Jac(self,x,B,d):\n",
    "        x.requires_grad_(True)\n",
    "        B.requires_grad_(True)\n",
    "        out = self.phi(x,B)\n",
    "        grad_x = torch.autograd.grad(outputs=out, inputs=x, grad_outputs=None, create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0] #first get grad using autograd\n",
    "        gradvp = torch.tensordot(grad_x,d,dims=([0,1],[0,1])).reshape(-1)\n",
    "        jvp = torch.autograd.grad(outputs=gradvp, inputs=B, grad_outputs=torch.ones(gradvp.shape).requires_grad_(True), create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "        return jvp.detach()\n",
    "    def CG(self,x,B,b,tol):\n",
    "        iteration = 0\n",
    "        r = b\n",
    "        p = r\n",
    "        rsold = float((torch.linalg.norm(r)**2).item())\n",
    "        solution = torch.zeros(x.shape)\n",
    "        while torch.linalg.norm(r)>tol and iteration < 2000:\n",
    "            Ap = torch.tensor(self.Hess(x,B,p), dtype=torch.float32)\n",
    "            alpha = rsold/torch.tensordot(p,Ap, dims=([0,1],[0,1]))\n",
    "            solution = solution + alpha*p\n",
    "            r = r - alpha*Ap\n",
    "            rsnew = torch.linalg.norm(r)**2\n",
    "            if torch.sqrt(rsnew) < tol:\n",
    "                return solution\n",
    "            p = r + (rsnew/rsold)*p.clone().detach()\n",
    "            rsold = rsnew\n",
    "            iteration += 1\n",
    "        return solution\n",
    "    def lbfgs(self,x,B, tol, max_iter= 10000):\n",
    "        shape = x.shape\n",
    "        x = x.flatten().clone().detach().requires_grad_(True)\n",
    "        lbfgs_optimiser= torch.optim.LBFGS([x], lr=0.05, max_iter=max_iter, max_eval=None, tolerance_grad=tol, tolerance_change=tol, history_size=100, line_search_fn=None)\n",
    "        def closure():\n",
    "            lbfgs_optimiser.zero_grad()\n",
    "            loss = self.phi(x.reshape(shape),B)\n",
    "            loss.backward(retain_graph=True)\n",
    "            return loss\n",
    "        lbfgs_optimiser.step(closure)\n",
    "        return x.reshape(shape)\n",
    "    def Upper_level(self,x_hat):\n",
    "        print (\"dicrepancy \", torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta))\n",
    "        if self.modified:\n",
    "            return 0.5 * torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta)**2+ 0.05*torch.linalg.norm((-torch.eye(self.A.shape[0])+self.B))**2\n",
    "        return 0.5 * torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta)**2+ 0.05*torch.linalg.norm((-self.A+self.B))**2\n",
    "    def solver(self):\n",
    "        stochastic = False\n",
    "        eps = self.eps\n",
    "        x_hat = self.x0\n",
    "        Bk = torch.nn.parameter.Parameter(data=self.B.clone().detach(), requires_grad=True)\n",
    "        optimiser = torch.optim.Adam([Bk], lr= 7*1e-4, betas=(0.9, 0.999), eps=1e-10, weight_decay=0, amsgrad=True)\n",
    "        # optimiser = torch.optim.SGD([Bk], lr= 9*1e-4, momentum=0.9, dampening=0, weight_decay=0, nesterov=True)\n",
    "        dp = torch.linalg.norm(torch.matmul(self.A,self.x0)- self.ydelta)\n",
    "        print (\"initial discrepancy \", dp)\n",
    "        for k in range(self.max_iter):\n",
    "            x_hat = self.lbfgs(x_hat,Bk, eps)\n",
    "            losslist.append(self.Upper_level(x_hat))\n",
    "            if k%10 == 0:\n",
    "                print('loss at iteration', k, 'is', losslist[-1].item())\n",
    "            q = self.CG(x_hat,Bk,torch.matmul(self.A.transpose(0,1),torch.matmul(self.A,x_hat)-self.ydelta).squeeze(0).squeeze(0),eps)\n",
    "            p = - self.Jac(x_hat,Bk,q) + 0.1*Bk.T@(-torch.eye(self.A.shape[0])+Bk)\n",
    "\n",
    "            if stochastic:\n",
    "                with torch.no_grad():\n",
    "                    param_shape = Bk.shape\n",
    "                    Bk.grad = (p.reshape(param_shape))\n",
    "                optimiser.step(lambda : self.Upper_level(x_hat))\n",
    "                if (k+1)%20 == 0:\n",
    "                    optimiser.param_groups[0]['lr'] *= 0.9\n",
    "            else:\n",
    "                if self.inexact:\n",
    "                    eps = max(eps * 0.9, 1e-12)\n",
    "                stepsize = 1e-5\n",
    "                Bk = Bk - stepsize * p\n",
    "            if torch.linalg.norm(torch.matmul(self.A,x_hat)- self.ydelta) <= dp:\n",
    "                break\n",
    "\n",
    "            self.B = Bk\n",
    "        return self.lbfgs(x_hat,Bk, 1e-14, max_iter=10000)\n",
    "\n",
    "input_size = 512\n",
    "x = Image.open('SheppLogan_Phantom.png')\n",
    "# x = Image.open('Cameraman.png')\n",
    "x = transforms.Resize((input_size, input_size))(x)\n",
    "x = transforms.Grayscale(num_output_channels=1)(x)\n",
    "x = transforms.ToTensor()(x).clamp(0, 1)\n",
    "plt.imshow(x[0].detach().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "h = 0.01\n",
    "t = torch.arange(-1 + h / 2, 1, h, dtype=torch.float)\n",
    "bsp = 0\n",
    "sigma = 3.0  # Adjust the blurring sigma here\n",
    "\n",
    "A = torch.zeros((x.shape[2], x.shape[2]), dtype=torch.float)\n",
    "for i in range(x.shape[2]):\n",
    "    for j in range(x.shape[2]):\n",
    "        A[i, j] = 1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-1 / (2 * sigma ** 2) * (i - j) ** 2)\n",
    "\n",
    "B = torch.eye(A.shape[0], dtype=torch.float)\n",
    "y = torch.matmul(A, x[0])\n",
    "eta = 0.005 * torch.randn(y.shape, dtype=y.dtype)\n",
    "print('noise:', torch.linalg.norm(eta))\n",
    "ydelta = y + eta\n",
    "\n",
    "torch.save(ydelta, 'ydelta.pt')\n",
    "\n",
    "plt.imshow(ydelta.detach().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "print('PSNR of noisy image is', psnr(x[0], ydelta))\n",
    "# Starting point\n",
    "M = torch.matmul(A.transpose(0, 1), A) + 0.0015 * torch.eye(x.shape[2])\n",
    "b = torch.matmul(A.transpose(0, 1), ydelta)\n",
    "x_tik = torch.linalg.solve(M, b)\n",
    "plt.imshow(x_tik.detach().numpy(), cmap='gray')\n",
    "plt.show()\n",
    "print('PSNR of Tikhonov is', psnr(x[0], x_tik).item())\n",
    "torch.save(x_tik, 'x_tik.pt')\n",
    "def psnr(x, x_hat):\n",
    "    mse = torch.mean((x-x_hat)**2)\n",
    "    return 10*torch.log10(1/mse)\n",
    "x_start = torch.clone(x_tik)\n",
    "model = ADP_IFT(A, B, x, x_start, ydelta, max_iter = 200, inexact = True, eps= 1e-1, modified = True)\n",
    "x_k = model.solver().detach().numpy()\n",
    "np.save('x_k_mod.npy', x_k)\n",
    "plt.imshow(x_k, cmap='gray')\n",
    "plt.show()\n",
    "# torch.save(model.B, 'B_mod_2d.pt')\n",
    "print('PSNR of modified ADP_IFT is', psnr(x,x_k).item())\n",
    "x_start = torch.clone(x_tik)\n",
    "model = ADP_IFT(A, B, x, x_start, ydelta, max_iter = 200, inexact = True, eps= 1e-1, modified = False)\n",
    "x_k = model.solver().detach().numpy()\n",
    "torch.save(model.B, 'B_2d.pt')\n",
    "np.save('x_k_ADP.npy', x_k)\n",
    "plt.imshow(x_k, cmap='gray')\n",
    "print('PSNR of ADP_IFT is', psnr(x,x_k).item())\n",
    "\n",
    "plot = True\n",
    "if plot:\n",
    "    # Create a figure and subplots\n",
    "    input_size = 256\n",
    "    x = Image.open('SheppLogan_Phantom.png')\n",
    "    x = transforms.Resize((input_size, input_size))(x)\n",
    "    x = transforms.Grayscale(num_output_channels=1)(x)\n",
    "    x = transforms.ToTensor()(x).clamp(0, 1)\n",
    "    image1 = x[0].detach().numpy()\n",
    "\n",
    "    image2 = torch.load('ydelta.pt').detach().numpy()\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "    # Disable axes ticks for both subplots\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot the first image in the first subplot\n",
    "    axes[0].imshow(image1)\n",
    "    axes[0].set_title('Ground truth')\n",
    "\n",
    "    # Plot the second image in the second subplot\n",
    "    axes[1].imshow(image2)\n",
    "    axes[1].set_title('Blurred image\\n PSNR = 23.95 dB')\n",
    "\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Ground_blur_phantom.png', dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "    image1 = torch.load('x_tik.pt').detach().numpy()\n",
    "    psnr1 = psnr(x[0], torch.from_numpy(image1)).item()\n",
    "    image2 = np.load('x_k_ADP.npy')\n",
    "    psnr2 = psnr(x[0], torch.from_numpy(image2)).item()\n",
    "    image3 = np.load('x_k_mod.npy')\n",
    "    psnr3 = psnr(x[0], torch.from_numpy(image3)).item()\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    # Disable axes ticks for both subplots\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    axes[0].imshow(image1, cmap='gray')\n",
    "    axes[0].set_title(f'Tikhonov\\n PSNR = {psnr1:.2f} dB')\n",
    "    axes[1].imshow(image2, cmap='gray')\n",
    "    axes[1].set_title(f'ADP\\n PSNR = {psnr2:.2f} dB')\n",
    "    axes[2].imshow(image3, cmap='gray')\n",
    "    axes[2].set_title(f'Modified ADP\\n PSNR = {psnr3:.2f} dB')\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Tik_ADP.png', dpi = 300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
